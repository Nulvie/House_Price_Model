{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j15R9YLFCH4w"
      },
      "source": [
        "### Part 0: Setup\n",
        "First, we need to import some libraries that are necessary to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQjoq-jB2Ms_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK2kI5hsCMza"
      },
      "source": [
        "Add additional modules/libraries to import here (rather than wherever you first use them below):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mfHQdMUCPEH",
        "outputId": "6df1c0eb-85d5-4ccc-8bd3-aab1bae64501"
      },
      "outputs": [],
      "source": [
        "# additional modules/libraries to import\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from plotnine import *\n",
        "from plotnine.data import *\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost\n",
        "!pip install tensorflow\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACJk5LbXh81q"
      },
      "source": [
        "Packages such as [TorchMetrics](https://torchmetrics.readthedocs.io/en/stable/) provide more options for evaluation metrics than what PyTorch natively provides. Here, we install the package and load their implementation of MSE, since it natively supports RMSE (which we will use in Part 2 instead of implementing it from scratch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7_tzfePh8ei",
        "outputId": "4a1cdc67-f499-409f-e307-25152c43fc0e"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGf9-IWnia8p"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import MeanSquaredError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjoglk9izfm2",
        "outputId": "d47a2fc8-f47d-4f59-fce8-d8c0adf42998"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "  device = \"cuda\"\n",
        "print(\"device: \" + device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UhnwXkpFLHN",
        "outputId": "4a4ae615-d0b0-4c59-d0a8-76f02f2a4e77"
      },
      "outputs": [],
      "source": [
        "# note that this command will trigger a request from google to allow colab\n",
        "# to access your files: you will need to accept the terms in order to access\n",
        "# the files this way\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# if you followed the instructions above exactly, your zipped data file should\n",
        "# be located at the file path below; if your files are in a different directory\n",
        "# on your Google Drive, you will need to change the path below accordingly\n",
        "ZIPPATH = '/content/drive/My Drive/comp341/comp341-hw7.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdod5xEAvfyq",
        "outputId": "2b1b6b6a-849e-421d-d937-299bb5688f16"
      },
      "outputs": [],
      "source": [
        "!cp \"{ZIPPATH}\" .\n",
        "!unzip -q \"comp341-hw7.zip\"\n",
        "!rm \"comp341-hw7.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkhteLBUNPjL"
      },
      "source": [
        "In your local colab instance, you should now have a `house_imgs/` directory with many images of homes (includes images from both the training and test sets), as well as two csv files: `home_data_train.csv` and `home_data_test.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHXaIt1qOwFN"
      },
      "source": [
        "### Part 1: Exploring the Home Images\n",
        "We have explored various tabular data extensively, especially in the context of dimensionality reduction when we have many features. One way to think about images is to consider each individual pixel (and channel) as an individual feature. Even with relatively small images like we have in our dataset, the dimensionality explodes pretty quickly, so let's explore if the dimensionality reduction methods we covered early on in the course can help us make sense of the data before we do any type of more sophisticated feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PSMIdaQ16g2"
      },
      "outputs": [],
      "source": [
        "# we provide some simple code to read in each training image and flatten\n",
        "# the pixel-based values to a tidy DataFrame, where each row is a house image\n",
        "# and each column is a feature (the R/G/B value at an individual pixel location)\n",
        "\n",
        "# get houseids for homes in the training dataset\n",
        "home_train = pd.read_csv('home_data_train.csv')\n",
        "img_ids = home_train['houseid'].astype(str).tolist()\n",
        "\n",
        "img_vect = []\n",
        "for idx in img_ids:\n",
        "  infile = os.path.join(\"house_imgs\", idx + \".jpg\")\n",
        "  file, ext = os.path.splitext(infile)\n",
        "  with Image.open(infile) as im:\n",
        "    img_vect.append(np.asarray(im).flatten())\n",
        "\n",
        "pixel_df = pd.DataFrame(np.vstack(img_vect))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBCKsCem3njG"
      },
      "source": [
        "Here, we store the flattened pixel values for each image, but we may want to look at the original image that corresponds to these flattened vectors. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXd9LjGLPoPd"
      },
      "outputs": [],
      "source": [
        "# TODO: write a simple function that, given a single house id as input (aka one of the elements\n",
        "# in img_ids), loads the image file from its location in LOCALDIR and displays it in your notebook directly\n",
        "# Hint: this is essentially a simpler version of the display_data function we provided in\n",
        "# Part 2 of the assignment, so you may be able to use some of the similar methods referenced\n",
        "# there as well as the provided HouseImagesDataset code\n",
        "\n",
        "def display_data(id):\n",
        "  #Loads the respective image with the given id and displays it\n",
        "  image = Image.open(\"house_imgs/\" + str(id) + \".jpg\")\n",
        "  display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "VFSM3qy36Mwr",
        "outputId": "f7353069-a18e-4eaa-9eb0-9a371273faaf"
      },
      "outputs": [],
      "source": [
        "# TODO: test your function on one of the images in img_ids\n",
        "display_data(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6mCrDVJ6ecw"
      },
      "source": [
        "Now, use a dimensionality reduction method that we covered in class that you think is appropriate for this problem. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wbP-7wxUG0ew",
        "outputId": "610cf490-8178-4422-f3b4-d9174d992493"
      },
      "outputs": [],
      "source": [
        "# TODO: calculate the 2D reduced dimensionality space and plot it (each image is a single point)\n",
        "# Note: depending on the dimensionality reduction method chosen,\n",
        "# this step can take a couple of minutes to complete\n",
        "\n",
        "PCA_pixel = pixel_df.copy(deep=True)\n",
        "\n",
        "#Uses PCA on the pixel df and plots the results\n",
        "pca = PCA(n_components=2)\n",
        "PCA_pixel = pd.DataFrame(pca.fit_transform(PCA_pixel), columns=['x', 'y'])\n",
        "\n",
        "(ggplot(PCA_pixel, aes(x='x', y='y'))\n",
        "+ geom_point()\n",
        "+ theme(figure_size=(12, 12))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xLXIs9pKaSI"
      },
      "source": [
        "If we look closely at some of our house images, we can see that instead of providing a \"real\" picture of the house, there are also schematics / floorplans. Two such examples are the houses at `houseid` 4112 and 7758. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "_GY7Bv_nKyht",
        "outputId": "d4db2934-60b4-4a70-955e-4067b63c0ea7"
      },
      "outputs": [],
      "source": [
        "# TODO: use your image display function to verify that these 2 houseids used drawings instead of real pictures of the house(s).\n",
        "display_data(4112)\n",
        "display_data(7758)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yOHn8DupI9El",
        "outputId": "ebdadcac-5c9f-4de0-8855-71b5ce60e61d"
      },
      "outputs": [],
      "source": [
        "# TODO: color the 2 points that correspond to these 2 houses in a new 2D reduced dimensionality plot\n",
        "\n",
        "#Finds the index of the 2 given house ids \n",
        "idx_4112 = home_train.index[home_train['houseid'] == 4112][0]\n",
        "idx_7758 = home_train.index[home_train['houseid'] == 7758][0]\n",
        "\n",
        "PCA_pixel_drawings = PCA_pixel.copy(deep=True)\n",
        "\n",
        "#Adds a boolean column to differentiate the previous house ids\n",
        "PCA_pixel_drawings['drawing'] = False\n",
        "PCA_pixel_drawings.iloc[idx_4112, 2] = True\n",
        "PCA_pixel_drawings.iloc[idx_7758, 2] = True\n",
        "\n",
        "(ggplot(PCA_pixel_drawings, aes(x='x', y='y', color='drawing'))\n",
        "+ geom_point()\n",
        "+ theme(figure_size=(12, 12))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jHOmtZXyLiiw",
        "outputId": "71f7e7ac-81fa-479c-fe10-04db06a9175c"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Goes through a sample of about 40 images and labels them to get training data\n",
        "drawing_idxs = [idx_4112, idx_7758, 50, 596, 752, 1743]\n",
        "drawings = PCA_pixel[PCA_pixel.index.isin(drawing_idxs)]\n",
        "drawings['drawing'] = 1\n",
        "\n",
        "nondrawing_idxs = [109, 330, 347, 537, 587, 614, 1284, 1556, 1557, 1558, 1559, 1616, 1626, 1793, 1916, 1917, 1924, 1928, 1982, \n",
        "                   1984, 2020, 2046, 2091, 2119, 2123, 2184, 2206, 2258, 2359, 2579, 2740, 2769, 2782, 2857, 2961, 2981]\n",
        "nondrawings = PCA_pixel[PCA_pixel.index.isin(nondrawing_idxs)]\n",
        "nondrawings['drawing'] = 0\n",
        "\n",
        "#Using logistic regression to classify the rest of the data as either a drawing or not\n",
        "drawings = pd.concat([drawings, nondrawings])\n",
        "y_drawing = drawings.pop('drawing')\n",
        "X_drawing = drawings\n",
        "\n",
        "clf = LogisticRegression().fit(X_drawing, y_drawing)\n",
        "pred_drawings = pd.DataFrame(clf.predict(PCA_pixel), columns=['pred'])\n",
        "\n",
        "#There are about 49 houses in the training data that have a drawing as their primary image.\n",
        "\n",
        "#Displays the images that the model predicts as a drawing\n",
        "for i in pred_drawings.loc[pred_drawings.pred == 1].index:\n",
        "  print(i, \": \")\n",
        "  id = home_train.iloc[i, 0]\n",
        "  display_data(id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD4eL3gIPATm"
      },
      "source": [
        "### Part 2: Predicting List Price\n",
        "Now that we also have some sense of what the house listing images are like based on Part 1, we will set up a regression framework that can use both the tabular features and image features. Along the way, we will also see how our predictions may change depending on what data we use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eACNOKbFRxRo"
      },
      "source": [
        "We provide several helper functions for setting up the data and functionality to visualize individual examples, which can sometimes be helpful to get a sense of what the model is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2zTWvbf0cvI"
      },
      "outputs": [],
      "source": [
        "# torch converts the 0-255 RGB values to 0-1 tensors, but it can\n",
        "# also be beneficial to also standardize the values (or, as we\n",
        "# see here, subtract the mean RGB values from the images)\n",
        "\n",
        "# these transformations below help facilitate this\n",
        "# inv_normalize is provided mainly for visualization sake, so that \n",
        "# we can flip the standardization process to see the image in its \n",
        "# original colors\n",
        "\n",
        "house_mean = [0.5230, 0.5416, 0.4989]\n",
        "# house_sd = [0.2271, 0.2162, 0.2640]\n",
        "# only subtracting mean and not also dividing by standard deviation\n",
        "# can actually sometimes work better, which is what we are doing here\n",
        "house_sd = [1, 1, 1]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(house_mean, house_sd)\n",
        "])\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(house_mean, house_sd)],\n",
        "   std= [1/s for s in house_sd]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25HEIj2Z0dqw"
      },
      "outputs": [],
      "source": [
        "# convenient function for displaying images\n",
        "# by default, will reverse the standardization calculation so that we can\n",
        "# see the images in a \"normal\" color scheme\n",
        "def display_data(d, inv_norm=True):\n",
        "  if isinstance(d['houseid'], list): # we can handle a list of houses\n",
        "    batch_size = len(d['houseid'])\n",
        "    for i in range(batch_size):\n",
        "      if 'price' in d:\n",
        "        print('price:', \"${:,.0f}\".format(d['price'][i]))\n",
        "\n",
        "      if inv_norm:\n",
        "       display(transforms.ToPILImage()(inv_normalize(d['image'][i])))\n",
        "      else:\n",
        "        display(transforms.ToPILImage()(d['image'][i]))\n",
        "  else: # only an individual house to be displayed\n",
        "    if 'price' in d:\n",
        "      print('price:', \"${:,.0f}\".format(d['price']))\n",
        "\n",
        "    if inv_norm:\n",
        "      display(transforms.ToPILImage()(inv_normalize(d['image'])))\n",
        "    else:\n",
        "      display(transforms.ToPILImage()(d['image'])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t3gjZDJ0fwi"
      },
      "outputs": [],
      "source": [
        "class HouseImagesDataset(Dataset):\n",
        "    def __init__(self, annot_file, image_dir, train=True):\n",
        "        # the annotation file is tidy, aka each row is a unique observation in the dataset,\n",
        "        # but it is not yet clean, which you will address in the TODO below\n",
        "        df = pd.read_csv(annot_file)\n",
        "\n",
        "        #Cleaning / preprocessing of features in df\n",
        "\n",
        "        #Imputes the missing numeric values using KNN imputation\n",
        "        KNN_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
        "        num_df = df[['beds', 'baths', 'sqft', 'lot_size']]\n",
        "        num_df = pd.DataFrame(KNN_imputer.fit_transform(num_df), columns = num_df.columns)\n",
        "        #Scales the choice numeric features\n",
        "        scaler = StandardScaler()\n",
        "        num_df = pd.DataFrame(scaler.fit_transform(num_df), columns=num_df.columns)\n",
        "\n",
        "        # #Turns zipcodes into categorical features\n",
        "        # zip_cat = pd.Categorical(df[\"zipcode\"], categories=df[\"zipcode\"].unique().tolist())\n",
        "        # df = df.assign(zip_cat = zip_cat)\n",
        "        # #Imputes the missing categorical values using simple imputation of the most frequent value\n",
        "        # simp_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
        "        # cat_df = df[['property_type', 'zip_cat']]\n",
        "\n",
        "        #Imputes the missing categorical values using simple imputation of the most frequent value\n",
        "        simp_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
        "        cat_df = df[['property_type']]\n",
        "        cat_df = pd.DataFrame(simp_imputer.fit_transform(cat_df), columns = cat_df.columns)\n",
        "        #One-hot encodes choice categorical features\n",
        "        cat_df = pd.get_dummies(cat_df)\n",
        "\n",
        "        #Combines the scaled numeric features with the one-hot encoded categorical features\n",
        "        clean_df = pd.concat([num_df, cat_df], axis=1)\n",
        "        #Finally, adds the houseid and list_price features back\n",
        "        clean_df = pd.concat([df['houseid'], clean_df], axis=1)\n",
        "        if train:\n",
        "          clean_df = pd.concat([clean_df, df['list_price']], axis=1)\n",
        "\n",
        "        # TODO: fill in this feature_cols list with the column names of\n",
        "        # features you would like to use to predict list price (many of the columns \n",
        "        # will likely be transformed from the original data in annot_file)\n",
        "        self.house_annot = clean_df\n",
        "        self.feature_cols = list(self.house_annot.columns)\n",
        "        self.image_dir = image_dir\n",
        "        self.train=train\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: fill in this method (replacing pass) to return the length of the dataset\n",
        "        return len(self.house_annot)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # we have provided code that will load and transform the indexed (\"ith\") image\n",
        "        # as well as features specified earlier in self.feature_cols within the processed\n",
        "        # pandas DataFrame\n",
        "\n",
        "        img = Image.open(str(self.image_dir) + str(self.house_annot.loc[idx, 'houseid']) + str('.jpg'))\n",
        "        img = transform(img)\n",
        "\n",
        "        features = self.house_annot.loc[idx, self.feature_cols]\n",
        "        features = features.tolist()\n",
        "        features = torch.FloatTensor(features)\n",
        "\n",
        "        # depending on whether the Dataset is in training mode, we will have the price data or not\n",
        "        if self.train:\n",
        "            item = {'image': img,\n",
        "                    'houseid': self.house_annot.loc[idx, 'houseid'],\n",
        "                    'features': features,\n",
        "                    'price': torch.tensor(self.house_annot.loc[idx, 'list_price'], dtype=torch.float)}\n",
        "        else:\n",
        "            item = {'image': img,\n",
        "                    'houseid': self.house_annot.loc[idx, 'houseid'],\n",
        "                    'features': features}\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2cZ1GL7X535",
        "outputId": "fd324b7b-1a22-43a1-859f-6c036049db81"
      },
      "outputs": [],
      "source": [
        "# TODO: initialize the house dataset using the training data you were provided and check the length of the dataset\n",
        "house_dataset = HouseImagesDataset(\"home_data_train.csv\", \"house_imgs/\", train=True)\n",
        "\n",
        "print(house_dataset.__len__())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "BrmxyXdgYy6g",
        "outputId": "4c214221-225c-466b-9582-3b1d594074bd"
      },
      "outputs": [],
      "source": [
        "# TODO: check that the data loads properly by calling the provided display_data function with a specifically indexed\n",
        "# item in your house dataset (e.g., house_dataset[3])\n",
        "\n",
        "print(display_data(house_dataset.__getitem__(3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTo02BXG1RkE",
        "outputId": "7c55c21e-b996-44e6-82aa-5b2cf2e5b4f0"
      },
      "outputs": [],
      "source": [
        "# TODO: use the convenient torch.utils.data.random_split function to split your loaded dataset into training and \n",
        "# validation portions, using 75% of the data for training and 25% of the data for validation\n",
        "\n",
        "house_train, house_valid = torch.utils.data.random_split(house_dataset, lengths=[3000, 1000])\n",
        "\n",
        "print(house_train)\n",
        "print(house_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jiryassl7ua"
      },
      "outputs": [],
      "source": [
        "class HybridHouseNN(nn.Module):\n",
        "  def __init__(self): \n",
        "    super().__init__()\n",
        "    # TODO: set up some convolutional layers\n",
        "    # it is your choice as to how many convolutional blocks, as well as\n",
        "    # specifics within the blocks: activation function, pooling, etc\n",
        "\n",
        "    conv1 = nn.Conv2d(3, 8, kernel_size=5, stride=1, padding=0)\n",
        "    a1 = nn.ReLU()\n",
        "    pool1 = nn.MaxPool2d(kernel_size=8)\n",
        "\n",
        "    # TODO: set up an MLP for the tabular features that you will be\n",
        "    # inputting into this model\n",
        "\n",
        "    fc1 = nn.Linear(10, 16)\n",
        "    a2 = nn.ReLU()\n",
        "\n",
        "    # TODO: set up the final set of fully connected layers that\n",
        "    # takes as input the concatenated set of flattened convolution features\n",
        "    # together with the output of the MLP from the tabular features\n",
        "    # to eventually output a single non-negative prediction\n",
        "\n",
        "    fc2 = nn.Linear(116224, 2000)\n",
        "    a3 = nn.Sigmoid()\n",
        "    fc3 = nn.Linear(2000, 64)\n",
        "\n",
        "    self.image_list = nn.ModuleList([conv1, a1, pool1])\n",
        "    self.tabular_list = nn.ModuleList([fc1, a2])\n",
        "    self.combined_list = nn.ModuleList([fc2, a3, fc3])\n",
        "        \n",
        "  def forward(self, ximg, xfeats): \n",
        "    # TODO: write out the forward pass steps\n",
        "    # note that forward now has 2 inputs because we are using both\n",
        "    # images and non-image features separately at first, before\n",
        "    # merging them together for the final set of predictions\n",
        "    # Note: you may also need to adjust the shape of your final prediction\n",
        "    # so that it plays nice with the loss function etc.\n",
        "\n",
        "    #Passes the features through their respective layers\n",
        "    for func in self.image_list:\n",
        "      ximg = func(ximg)\n",
        "    for func in self.tabular_list:\n",
        "      xfeats = func(xfeats)\n",
        "    \n",
        "    #Flattens both sets of parameters\n",
        "    ximg = torch.flatten(ximg)\n",
        "    xfeats = torch.flatten(xfeats)\n",
        "\n",
        "    #Passes the combined features through the final set of layers\n",
        "    xcombined = torch.cat((ximg, xfeats))\n",
        "    for func in self.combined_list:\n",
        "      xcombined = func(xcombined)\n",
        "\n",
        "    return xcombined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTv3tpDspnha"
      },
      "source": [
        "Before training our model, we want to also set up some additional models to what the differences might be if we use *only* images or *only* the tabular features for our predictions. Of course, if we set the models up differently with different hyperparameters, we really cannot have a truly equivalent comparison, but we will try to keep as many of the model blocks the same as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrgu9qHUpa9P"
      },
      "outputs": [],
      "source": [
        "class HouseImageOnly(nn.Module):\n",
        "  def __init__(self): \n",
        "    super().__init__()\n",
        "\n",
        "    #Convolutional Layer\n",
        "    conv1 = nn.Conv2d(3, 8, kernel_size=5, stride=1, padding=0)\n",
        "    a1 = nn.ReLU()\n",
        "    pool1 = nn.MaxPool2d(kernel_size=8)\n",
        "\n",
        "    #Fully connected layers\n",
        "    fc1 = nn.Linear(115200, 2000)\n",
        "    a2 = nn.Sigmoid()\n",
        "    fc2 = nn.Linear(2000, 64)\n",
        "\n",
        "    self.conv_pool_list = nn.ModuleList([conv1, a1, pool1])\n",
        "    self.fcl_list = nn.ModuleList([fc1, a2, fc2])\n",
        "        \n",
        "  def forward(self, ximg): \n",
        "    #Pass the image's parameters through the convolutional layers\n",
        "    for func in self.conv_pool_list:\n",
        "      ximg = func(ximg)\n",
        "\n",
        "    #Flatten and pass the parameters through the fully connected layers\n",
        "    ximg = torch.flatten(ximg)\n",
        "    for func in self.fcl_list:\n",
        "      ximg = func(ximg)\n",
        "\n",
        "    return ximg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVkdwDSqq0TF"
      },
      "outputs": [],
      "source": [
        "class HouseFeatsOnly(nn.Module):\n",
        "  def __init__(self): \n",
        "    super().__init__()\n",
        "\n",
        "    #First layer from the MLP\n",
        "    fc1 = nn.Linear(10, 16)\n",
        "    a1 = nn.ReLU()\n",
        "\n",
        "    #Fully connected layers\n",
        "    fc2 = nn.Linear(1024, 2000)\n",
        "    a2 = nn.Sigmoid()\n",
        "    fc3 = nn.Linear(2000, 64)\n",
        "\n",
        "    self.mlp_list = nn.ModuleList([fc1, a1])\n",
        "    self.fcl_list = nn.ModuleList([fc2, a2, fc3])\n",
        "        \n",
        "  def forward(self, xfeats): \n",
        "    #Pass the image's parameters through the MLP\n",
        "    for func in self.mlp_list:\n",
        "      xfeats = func(xfeats)\n",
        "\n",
        "    #Flatten and pass the parameters through the fully connected layers\n",
        "    xfeats = torch.flatten(xfeats)\n",
        "    for func in self.fcl_list:\n",
        "      xfeats = func(xfeats)\n",
        "\n",
        "    return xfeats   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmLTwRqzrk4E"
      },
      "source": [
        "As mentioned earlier, we will use RMSE for our loss function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFAR9wXyrkKv"
      },
      "outputs": [],
      "source": [
        "loss_fn = MeanSquaredError(squared=False).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SUJKo1NwOcv"
      },
      "source": [
        "Now, let's fill in the details of the training and validation methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVn4_u-Jr2pU"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, opt, epoch, mode=\"both\", verbose=False):\n",
        "  # mode can be \"both\", \"image\", or \"features\", depending on if we are using\n",
        "  # our HybridHouseNN, HouseImageOnly, or HouseFeatsOnly model\n",
        "  # we will assume that the model passed to this function matches the mode,\n",
        "  # and mode will affect whether the model uses image, features, or a combination\n",
        "  # as input to get the predictions in the forward pass\n",
        "  \n",
        "  if verbose:\n",
        "    print(\"starting epoch\", epoch)\n",
        "  train_loss = 0\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    image, features, price = batch['image'].to(device), \\\n",
        "                             batch['features'].to(device), \\\n",
        "                             batch['price'].to(device)\n",
        "\n",
        "    model.train(True)\n",
        "\n",
        "    # TODO: fill in the code for each of the steps in the\n",
        "    # training loop, remembering that we want to account for\n",
        "    # different modes in the forward pass step\n",
        "    pred = 0\n",
        "    if mode == \"both\":\n",
        "      pred = model(image, features)\n",
        "    elif mode == \"image\":\n",
        "      pred = model(image)\n",
        "    else:\n",
        "      pred = model(features)\n",
        "    #Calculate loss\n",
        "    loss = loss_fn(pred, price)\n",
        "    #Backward pass\n",
        "    loss.backward()\n",
        "    #Update weight estimates\n",
        "    opt.step()\n",
        "    #Reset gradients to zero\n",
        "    opt.zero_grad()\n",
        "    #We are tracking the sum of losses to calculate the average training loss for this epoch\n",
        "    train_loss += loss.item()\n",
        "  \n",
        "    model.train(False)\n",
        "    if verbose and ((i % 20) == 0):\n",
        "      print('training [epoch {}: {}/{} ({:.0f}%)] loss: {:.6f}'.format(\n",
        "          epoch, i * len(image), len(train_loader.dataset),\n",
        "          100. * i / len(train_loader), loss.item()))\n",
        "    if (i % 20) == 0:\n",
        "    #log the running loss\n",
        "      writer.add_scalar('training loss',\n",
        "                        train_loss / 20,\n",
        "                        epoch * len(train_loader) + i)\n",
        "\n",
        "  avg_tl = train_loss / (i+1)\n",
        "  print('epoch {} avg training loss: {:.6f}'.format(epoch, avg_tl))\n",
        "  return avg_tl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRVRECDJwJSh"
      },
      "outputs": [],
      "source": [
        "def valid(model, valid_loader, mode=\"both\"):\n",
        "  # as in train, mode can be \"both\", \"image\", or \"features\", depending on if we are using\n",
        "  # our HybridHouseNN, HouseImageOnly, or HouseFeatsOnly model\n",
        "  # we will assume that the model passed to this function matches the mode,\n",
        "  # and mode will affect whether the model uses image, features, or a combination\n",
        "  # as input to get the predictions in the forward pass\n",
        "  valid_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(valid_loader):\n",
        "      image, features, price = batch['image'].to(device), \\\n",
        "                               batch['features'].to(device), \\\n",
        "                               batch['price'].to(device)\n",
        "      \n",
        "      # TODO: fill in code to calculate pred (the prediction), paying attention to \n",
        "      # different usage of the model depending on the inputted mode variable\n",
        "      pred = 0\n",
        "      if mode == \"both\":\n",
        "        pred = model(image, features)\n",
        "      elif mode == \"image\":\n",
        "        pred = model(image)\n",
        "      else:\n",
        "        pred = model(features)\n",
        "      \n",
        "      #Get loss\n",
        "      valid_loss += loss_fn(pred, price).item()\n",
        "\n",
        "\n",
        "  # get the loss for the epoch\n",
        "  avg_vl = valid_loss / (i+1)\n",
        "  print('avg validation loss: {:.6f}'.format(avg_vl))\n",
        "  \n",
        "  return avg_vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIEw13jwwu4z"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(house_train, batch_size = batch_size, shuffle=True, drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(house_valid, batch_size = batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jreS4J0xBlY",
        "outputId": "14b398e1-28e0-4bb1-dcfc-dfee6df8f5ff"
      },
      "outputs": [],
      "source": [
        "epoch_list = defaultdict(list)\n",
        "train_loss = defaultdict(list)\n",
        "valid_loss = defaultdict(list)\n",
        "\n",
        "#I only did 10 epochs because this takes me hours to run.\n",
        "epochs = 10\n",
        "modes = {'both': HybridHouseNN(),'image': HouseImageOnly(),'features': HouseFeatsOnly()}\n",
        "for m in modes:\n",
        "  model = modes[m].to(device)\n",
        "  # TODO: initialize the optimizer (and associated hyperparameters like learning rate) of your choice\n",
        "  opt = torch.optim.Adam(model.parameters(), lr=.1)\n",
        "\n",
        "  print(\"Current mode:\", m)\n",
        "  for e in range(1, epochs+1):\n",
        "    epoch_list[m].append(e)\n",
        "    train_loss[m].append(train(model, train_loader, opt, e, m, verbose=True))\n",
        "    valid_loss[m].append(valid(model, valid_loader, m))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "3CbqfpVewsC7",
        "outputId": "aed55c1f-8f83-4f74-ab35-d6fad4205747"
      },
      "outputs": [],
      "source": [
        "#Gets the number of epochs\n",
        "epochs = pd.DataFrame(epoch_list['both'], columns=['epochs'])\n",
        "\n",
        "#Concats the number of epochs, training loss, and validation loss\n",
        "train_losses = pd.DataFrame(train_loss).rename(columns={'both': 'train_both', 'image': 'train_image', 'features': 'train_features'})\n",
        "valid_losses = pd.DataFrame(valid_loss).rename(columns={'both': 'valid_both', 'image': 'valid_image', 'features': 'valid_features'})\n",
        "losses = pd.concat([epochs, train_losses, valid_losses], axis=1)\n",
        "\n",
        "#Plots each loss type as a separate line with their own color\n",
        "(ggplot(losses, aes(x='epochs'))\n",
        "+ geom_line(aes(y='train_both', color=\"'red'\"))\n",
        "+ geom_line(aes(y='train_image', color=\"'orange'\"))\n",
        "+ geom_line(aes(y='train_features', color=\"'yellow'\"))\n",
        "+ geom_line(aes(y='valid_both', color=\"'blue'\"))\n",
        "+ geom_line(aes(y='valid_image', color=\"'green'\"))\n",
        "+ geom_line(aes(y='valid_features', color=\"'purple'\"))\n",
        "+ theme(figure_size=(12, 12))\n",
        "+ labs(y='Loss')\n",
        "+ scale_color_identity(guide='legend',name='Loss Type',\n",
        "                        breaks=['red', 'orange', 'yellow', 'blue', 'green', 'purple'],\n",
        "                        labels=['train_both','train_image','train_features', 'valid_both', 'valid_image', 'valid_features'])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm_yglACzU1f"
      },
      "source": [
        "Using both or only image is usually the best performing (it changes between runs). They have the lowest amount of loss in both training and validation sets. On the other hand, using only tabular features seems to perform very poorly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIR3aGYcNN7e"
      },
      "outputs": [],
      "source": [
        "# Reads in the training and test data\n",
        "home_train = pd.read_csv('home_data_train.csv')\n",
        "home_test = pd.read_csv('home_data_test.csv')\n",
        "\n",
        "\n",
        "def cleanNumeric(df):\n",
        "  #Imputes the missing numeric values using KNN imputation\n",
        "  KNN_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
        "  num_df = df[['beds', 'baths', 'sqft', 'lot_size']]\n",
        "  num_df = pd.DataFrame(KNN_imputer.fit_transform(num_df), columns = num_df.columns)\n",
        "  #Scales the choice numeric features\n",
        "  scaler = StandardScaler()\n",
        "  num_df = pd.DataFrame(scaler.fit_transform(num_df), columns=num_df.columns)\n",
        "  return num_df\n",
        "\n",
        "\n",
        "cat_train = home_train[['property_type']]\n",
        "cat_test = home_test[['property_type']]\n",
        "\n",
        "#Gets the categorical zipcodes\n",
        "zip_train = pd.Categorical(home_train[\"zipcode\"], categories=home_train[\"zipcode\"].unique().tolist())\n",
        "cat_train = cat_train.assign(zip_cat = zip_train)\n",
        "zip_test = pd.Categorical(home_test[\"zipcode\"], categories=home_test[\"zipcode\"].unique().tolist())\n",
        "cat_test = cat_test.assign(zip_cat = zip_test)\n",
        "\n",
        "#Imputes the missing categorical values using Simple Imputation of the most frequent value\n",
        "simp_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
        "cat_train = pd.DataFrame(simp_imputer.fit_transform(cat_train), columns = cat_train.columns)\n",
        "cat_test = pd.DataFrame(simp_imputer.fit_transform(cat_test), columns = cat_test.columns)\n",
        "\n",
        "#Makes sure both dataframes have the same columns\n",
        "temp = pd.get_dummies(pd.concat([cat_train[['property_type', 'zip_cat']], cat_test[['property_type', 'zip_cat']]],keys=[0,1]))\n",
        "cat_train, cat_test = temp.xs(0),temp.xs(1)\n",
        "\n",
        "#Combines the scaled numeric features with the one-hot encoded categorical features\n",
        "X_train = pd.concat([cleanNumeric(home_train), cat_train], axis=1)\n",
        "X_test = pd.concat([cleanNumeric(home_test), cat_test], axis=1)\n",
        "\n",
        "#Combines the tabular features with our PCA reduced image data\n",
        "train_images = PCA_pixel[['x', 'y']]\n",
        "X_train = pd.concat([train_images, X_train], axis=1)\n",
        "\n",
        "#Gets the PCA reduced image data of the test set\n",
        "img_ids = home_test['houseid'].astype(str).tolist()\n",
        "\n",
        "img_vect = []\n",
        "for idx in img_ids:\n",
        "  infile = os.path.join(\"house_imgs\", idx + \".jpg\")\n",
        "  file, ext = os.path.splitext(infile)\n",
        "  with Image.open(infile) as im:\n",
        "    img_vect.append(np.asarray(im).flatten())\n",
        "\n",
        "test_images = pd.DataFrame(np.vstack(img_vect))\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "test_images = pd.DataFrame(pca.fit_transform(test_images), columns=['x', 'y'])\n",
        "X_test = pd.concat([test_images, X_test], axis=1)\n",
        "\n",
        "y_train = home_train[['list_price']]\n",
        "\n",
        "#Uses XGBoost to model the list_prices\n",
        "xgb_reg = xgboost.XGBRegressor(booster='gbtree', eta=0.3, max_depth=6, objective='reg:squarederror', eval_metric='rmse')\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93wN7WBIc_Zh"
      },
      "outputs": [],
      "source": [
        "results = pd.Series(y_pred.flatten(), name=\"price\")\n",
        "results = pd.concat([home_test['houseid'], results], axis=1)\n",
        "results.to_csv('my_submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2C_lTqhMaT7g"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
